%TEX root = ../dissertation.tex

\chapter{Overview}
\label{chapter:chaptername}

CAD Based-Systems are typically single-user oriented that is, designed to support individual tasks such as notations and information visualisation. This personal and task-oriented approach for clinical software provides little support for the aggregation of resources and tools required in carrying out higher level activities for multimodality of medical imaging. It is left to the user to aggregate such resources and tools in meaningful bundles according to the activity at hand, and users often have to reconfigure this aggregation manually when shifting between a set of parallel activities and machines.

A  suited  number  of  studies  have  shown  that  clinical  professionals, upon the act of organising and thinking in their work routines, which often carried out search of general objectives, often in collaboration with others [???, ???, ???], are significant mental and manual overhead associated with handling of  parallel  work  and  interruptions [???,  ???]. The rest of the user  interfaces  in  the  current operating systems, fail to provide adequate support in the resumption of the previous  activities  and  for an  easy  switching  between  parallel  activities [???, ???].

Clinical user interfaces have been extensively discussed in the literature on information visualisation. MIMBCD-UI shows the details of an user interface for diagnosing breast cancer using multimodality medical imaging.

MIMBCD-UI have several benefits. In fact, the diagnosis it self is more efficient since the clinical users may navigate using the overview of multimodality of imaging rather than the others techniques. The overview of multimodality of imaging window aids users in keeping track of their current position in the information space [???]. Moreover the overview window itself give users task-relevant information and a feeling of control [???].

A multimodality of views permits to acquire better, more efficient and flexible information and to easily diagnose in it; however, it is more difficult to users to manage information in a more complex user interface.

Specifically, this project deals with the use of a recently proposed technique in literature: Deep Convolutional Neural Networks (CNNs).

These deep networks will incorporate information from several different modes: magnetic resonance imaging volumes (MRI), ultrasound images, mammographic images (both views CC and MLO) and text.

The proposed algorithm, called for multimodality CNNs (MMCNNs) will have the ability to process multimodal information at an unified and sustained manner.

This methodology needs to "learn" what are the masses and calcifications.

So that is necessary to collect the ground truth, or notes of the masses and calcifications provided by medical experts.

\break

For the collection of these notes, the design and development of an interface is necessary allows the user (in this case, the medical specialist) to display various types of image (i.e., ultrasound, MRI and mammography), and that also allows for user interaction, particularly in providing the notes of the masses and calcifications.

For these reasons, it is crucial for the development of this project, cooperation with experts providing the above notes.